class ETL:
    self.data_to_be_ingested
    self.create_agg_table

    def __init__(self, source, target):
        self.s3_source = source
        self.sf_target = target

    def data_ingest(self, self.s3_source, self.sf_target):

        global path
        self.data_to_be_ingested = spark.read.csv(path+self.s3_source,
                                                  escape='"',
                                                  multiLine=True,
                                                  inferSchema=False,
                                                  header=True)

        self.data_to_be_ingested.\
            repartition(5, 'sku').\
            write.csv(path+self.sf_target,
                      header=True,
                      mode='overwrite')
        return

    def create_agg_table(self, self.target):

        self.create_agg_table = self.data_to_be_ingested.\
                                groupBy("name").count().\
                                withColumnRenamed("count", "product_count")
        self.create_agg_table.repartition(5, 'name')\
            .write.csv(path+self.target, header=True, mode='overwrite')
        return









SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"


# Helper Function
def snowflake_options(sfUrl, sfUser, sfPassword,
                      schema: str = None,
                      sfDatabase: str = "postman_db",
                      table: str = None,
                      timestamp=False,
                      sfWarehouse: str = "COMPUTE_WH"):
    assert schema
    assert table
    sfOptions = {
        "sfURL": sfUrl,
        "sfUser": sfUser,
        "sfPassword": sfPassword,
        "sfDatabase": sfDatabase,
        "sfSchema": schema,
        "sfWarehouse": sfWarehouse,
        "parallelism": 8,
        "dbtable": table
        }
    return sfOptions


# data read from csv & write as parquet with repartition by sku
def data_ingest(source_path, target):
    data_to_be_ingested = spark.read.csv(source_path,
                                         escape='"',
                                         multiLine=True,
                                         inferSchema=False,
                                         header=True)
    data_to_be_ingested.\
        repartition(5, 'sku').\
        write.csv(target,
                  header=True,
                  mode='overwrite')
    return


# creating an aggregate table to list
def create_agg_table(data_source, destination):
    table = spark.read.csv(data_source, header=True)
    aggr = table.groupBy("name").count()\
                .withColumnRenamed("count", "product_count")
    aggr.repartition(5, 'name')\
        .write.csv(destination, header=True, mode='overwrite')
    return aggr


path = "s3://postmantestde"
create_agg_table(data_source=f'{path}/partitiondata/',
                 dest=f'{path}/agg_table/')



to_snowflake = spark.read.csv(path+'/repartitioned_csv/',
                              escape='"',
                              multiLine=True,
                              inferSchema=False,
                              header=True)

to_snowflake.write\
            .format(SNOWFLAKE_SOURCE_NAME)\
            .options(**snowflake_options(
                sfUrl='yxa46258.us-east-1.snowflakecomputing.com/',
                sfUser='Postman_team',
                sfPassword='Postman_team_20',
                schema='POC',
                sfDatabase="postman_db",
                table='products'))\
        .mode("overwrite").save(partitionBy="sku")


products = spark\
    .read.format(SNOWFLAKE_SOURCE_NAME)\
    .options(**snowflake_options(
                sfUrl='yxa46258.us-east-1.snowflakecomputing.com/',
                sfUser='Postman_team',
                sfPassword='Postman_team_20',
                schema='POC',
                sfDatabase="postman_db",
                table='products')).load()
